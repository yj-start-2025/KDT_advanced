{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## DeepLearning Skills\n",
        "- 모델의 overfitting 을 해결하기 위해?\n",
        "  - Regularization\n",
        "  - Early stopping"
      ],
      "metadata": {
        "id": "OO3ZMAUqLmuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 01 라이브러리 불러오기"
      ],
      "metadata": {
        "id": "i5F3lsVzLv0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "classification_data_path = \"./sample_data/\"\n",
        "\n",
        "validation_data_ratio = 0.2\n",
        "\n",
        "random_state = 0\n",
        "\n",
        "classifier_input_size = 28 * 28\n",
        "classifier_hidden_size1 = 512\n",
        "classifier_hidden_size2 = 128\n",
        "classifier_hidden_size3 = 32\n",
        "classifier_output_size = 10\n",
        "\n",
        "l1_penalty = 1e-4\n",
        "l2_penalty = 1e-4\n",
        "\n",
        "dropout_rate = 0.2\n",
        "\n",
        "learning_rate = 5e-3\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "classification_criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "nUEuJl_tLtwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 02 데이터 불러오는 클래스 정의"
      ],
      "metadata": {
        "id": "3JoHNnBhLxt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationDataLoader:\n",
        "    def __init__(self, data_path, validation_data_ratio, batch_size):\n",
        "        self.data_path = data_path\n",
        "        self.validation_data_ratio = validation_data_ratio\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transforms.ToTensor()\n",
        "\n",
        "    def __call__(self, flag):\n",
        "        if flag == \"train\":\n",
        "            dataset = FashionMNIST(\n",
        "                self.data_path, train=True, download=True, transform=self.transform\n",
        "            )\n",
        "\n",
        "            train_data_size = int(len(dataset) * (1 - self.validation_data_ratio))\n",
        "            validation_data_size = len(dataset) - train_data_size\n",
        "\n",
        "            train_dataset, validation_dataset = random_split(\n",
        "                dataset, [train_data_size, validation_data_size]\n",
        "            )\n",
        "\n",
        "            train_loader = DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=True,\n",
        "                drop_last=True,\n",
        "                num_workers=0,\n",
        "            )\n",
        "            validation_loader = DataLoader(\n",
        "                validation_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=True,\n",
        "                drop_last=True,\n",
        "                num_workers=0,\n",
        "            )\n",
        "\n",
        "            return train_loader, validation_loader\n",
        "        else:\n",
        "            test_dataset = FashionMNIST(\n",
        "                self.data_path, train=False, download=True, transform=self.transform\n",
        "            )\n",
        "            test_loader = DataLoader(\n",
        "                test_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=False,\n",
        "                drop_last=True,\n",
        "                num_workers=0,\n",
        "            )\n",
        "            return test_loader\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nByfndIDL0rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_loader = ClassificationDataLoader(\n",
        "    classification_data_path, validation_data_ratio, batch_size\n",
        ")\n",
        "classification_train_loader, classification_validation_loader = classification_loader(\n",
        "    \"train\"\n",
        ")\n",
        "classification_test_loader = classification_loader(\"test\")"
      ],
      "metadata": {
        "id": "jz0pLLa1Rgnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 03 Deep Learning 모델"
      ],
      "metadata": {
        "id": "jawu7WwrL1Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNeuralNetwork(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size\n",
        "    ):\n",
        "        super(DeepNeuralNetwork, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size1 = hidden_size1\n",
        "        self.hidden_size2 = hidden_size2\n",
        "        self.hidden_size3 = hidden_size3\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(self.input_size, self.hidden_size1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_size1, self.hidden_size2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_size2, self.hidden_size3),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.output_layer = nn.Linear(self.hidden_size3, self.output_size)\n",
        "\n",
        "    def forward(self, data):\n",
        "        data = self.prepare_input(data)\n",
        "        feature = self.feature_extractor(data)\n",
        "        result = self.output_layer(feature)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def prepare_input(self, data):\n",
        "        flattened_data = data.view(data.size(0), -1)\n",
        "\n",
        "        return flattened_data\n",
        "\n",
        "\n",
        "class DropOutDNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        dropout_rate,\n",
        "        hidden_size1,\n",
        "        hidden_size2,\n",
        "        hidden_size3,\n",
        "        output_size,\n",
        "    ):\n",
        "        super(DropOutDNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.hidden_size1 = hidden_size1\n",
        "        self.hidden_size2 = hidden_size2\n",
        "        self.hidden_size3 = hidden_size3\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(self.input_size, self.hidden_size1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(self.hidden_size1, self.hidden_size2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "            nn.Linear(self.hidden_size2, self.hidden_size3),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout_rate),\n",
        "        )\n",
        "        self.output_layer = nn.Linear(self.hidden_size3, self.output_size)\n",
        "\n",
        "    def forward(self, data):\n",
        "        data = self.prepare_input(data)\n",
        "        feature = self.feature_extractor(data)\n",
        "        result = self.output_layer(feature)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def prepare_input(self, data):\n",
        "        flattened_data = data.view(data.size(0), -1)\n",
        "\n",
        "        return flattened_data\n",
        "\n",
        "\n",
        "classifier = DeepNeuralNetwork(\n",
        "    classifier_input_size,\n",
        "    classifier_hidden_size1,\n",
        "    classifier_hidden_size2,\n",
        "    classifier_hidden_size3,\n",
        "    classifier_output_size,\n",
        ").to(device)\n",
        "dropout_classifier = DropOutDNN(\n",
        "    classifier_input_size,\n",
        "    dropout_rate,\n",
        "    classifier_hidden_size1,\n",
        "    classifier_hidden_size2,\n",
        "    classifier_hidden_size3,\n",
        "    classifier_output_size,\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "TDinbBgaL5Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_optimizer = torch.optim.SGD(\n",
        "    classifier.parameters(), lr=learning_rate, weight_decay=l2_penalty\n",
        ")\n",
        "dropout_classifier_optimizer = torch.optim.SGD(\n",
        "    dropout_classifier.parameters(), lr=learning_rate, weight_decay=l2_penalty\n",
        ")"
      ],
      "metadata": {
        "id": "_gC9AOf0SNvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 04 Regularization(규제)"
      ],
      "metadata": {
        "id": "psSjf3AgL6jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path=\"checkpoint.pt\"):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(\n",
        "                \"EarlyStopping counter: {} out of {}\".format(\n",
        "                    self.counter, self.patience\n",
        "                )\n",
        "            )\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"Test loss decreased ({:.6f}) --> {:.6f}. Saving model\".format(\n",
        "                    self.val_loss_min, val_loss\n",
        "                )\n",
        "            )\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n"
      ],
      "metadata": {
        "id": "_Wei45cnL8iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 05 Test하기"
      ],
      "metadata": {
        "id": "kHowsx2RL-v8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        train_data_loader,\n",
        "        validation_data_loader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        epochs,\n",
        "        device,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.train_data_loader = train_data_loader\n",
        "        self.validation_data_loader = validation_data_loader\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.epochs = epochs\n",
        "        self.device = device\n",
        "        self.early_stopping = EarlyStopping()\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            for data, label in self.train_data_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                data, label = data.to(self.device), label.to(self.device)\n",
        "                result = self.model(data)\n",
        "                loss = self.criterion(result, label)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            val_loss = self.get_valid_loss()\n",
        "            print(\n",
        "                \"epoch: {} / {}, validation loss: {}\".format(\n",
        "                    epoch, self.epochs, val_loss\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.early_stopping(val_loss, self.model)\n",
        "            if self.early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    def train_l1_regularizer(self):\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            for data, label in self.train_data_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                data, label = data.to(self.device), label.to(self.device)\n",
        "                result = self.model(data)\n",
        "\n",
        "                loss = self.criterion(result, label)\n",
        "                l1_loss = torch.FloatTensor([0]).to(device)\n",
        "                for param in self.model.parameters():\n",
        "                    l1_loss += torch.sum(torch.abs(param))\n",
        "\n",
        "                loss = loss + l1_penalty * l1_loss\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            val_loss = self.get_valid_loss()\n",
        "            print(\n",
        "                \"epoch: {} / {}, validation loss: {}\".format(\n",
        "                    epoch, self.epochs, val_loss\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.early_stopping(val_loss, self.model)\n",
        "            if self.early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    def get_valid_loss(self):\n",
        "        self.model.eval()\n",
        "        valiation_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for data, label in self.validation_data_loader:\n",
        "                data, label = data.to(self.device), label.to(self.device)\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output, label)\n",
        "                valiation_loss += loss.item()\n",
        "\n",
        "        return valiation_loss / len(self.validation_data_loader)\n",
        "\n",
        "\n",
        "classifier_trainer = Trainer(\n",
        "    classifier,\n",
        "    classification_train_loader,\n",
        "    classification_validation_loader,\n",
        "    classifier_optimizer,\n",
        "    classification_criterion,\n",
        "    epochs,\n",
        "    device,\n",
        ")\n",
        "classifier_trainer.train()\n",
        "\n",
        "dropout_classifier_trainer = Trainer(\n",
        "    dropout_classifier,\n",
        "    classification_train_loader,\n",
        "    classification_validation_loader,\n",
        "    dropout_classifier_optimizer,\n",
        "    classification_criterion,\n",
        "    epochs,\n",
        "    device,\n",
        ")\n",
        "dropout_classifier_trainer.train()\n"
      ],
      "metadata": {
        "id": "1S16DQ6iMABz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}